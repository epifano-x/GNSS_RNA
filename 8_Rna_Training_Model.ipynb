{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8_Rna_Training_Model.ipynb\n",
    "\n",
    "## Visão Geral\n",
    "\n",
    "Este notebook tem como objetivo treinar um modelo de Rede Neural Artificial (RNA) para prever diferenças de latitude e longitude com base em dados de satélites. O processo envolve várias etapas, desde o carregamento e preparação dos dados até a construção, treinamento e avaliação do modelo. A seguir, uma descrição detalhada das principais seções e funcionalidades implementadas no notebook.\n",
    "\n",
    "## Estrutura do Notebook\n",
    "\n",
    "### Bloco 1: Imports e Configurações\n",
    "\n",
    "- **Importação de Bibliotecas**: Carrega todas as bibliotecas necessárias, incluindo `numpy`, `pandas`, `matplotlib`, `tensorflow`, `sklearn`, entre outras.\n",
    "- **Configurações Iniciais**: Define caminhos para os arquivos JSON de entrada e saída, parâmetros de treinamento como tamanho do lote (`BATCH_SIZE`), número de épocas (`EPOCHS`), proporção de validação (`VAL_SPLIT`), e outros parâmetros relevantes.\n",
    "\n",
    "### Bloco 2: Funções Utilitárias\n",
    "\n",
    "- **Descoberta de Chaves das Features**: Função para identificar todas as chaves (features) presentes no arquivo JSON, excluindo as colunas alvo (`TARGET_COLS`).\n",
    "- **Conversão de Dicionário para Arrays NumPy**: Transforma cada registro do JSON em arrays NumPy (`X` para features e `y` para targets), garantindo que todas as features sejam representadas de forma consistente.\n",
    "- **Gerador de Dataset com ijson**: Utiliza a biblioteca `ijson` para processar o arquivo JSON de forma eficiente e gerar pares `(X, y)` para o treinamento.\n",
    "- **Criação do Dataset do TensorFlow**: Constrói um `tf.data.Dataset` para streaming dos dados, com funcionalidades de embaralhamento, agrupamento em lotes e prefetching para otimização do treinamento.\n",
    "\n",
    "### Bloco 3: Construção do Modelo com Blocos Residenciais\n",
    "\n",
    "- **Definição de Blocos Residenciais**: Implementa blocos residuais que ajudam a mitigar o problema de gradientes desaparecidos, permitindo a construção de modelos mais profundos.\n",
    "- **Função de Construção do Modelo (`build_model`)**: Monta a arquitetura completa da RNA, incluindo camadas densas iniciais, múltiplos blocos residuais, camadas intermediárias e a camada de saída com ativação linear para prever as diferenças de latitude e longitude.\n",
    "\n",
    "### Bloco 4: Função Principal - Treino, Fine-tuning, Gráficos e Predições\n",
    "\n",
    "- **Descoberta e Preparação das Features**: Identifica as chaves das features no dataset e prepara o dataset para treinamento.\n",
    "- **Carregamento e Streaming dos Dados**: Utiliza o `tf.data.Dataset` para carregar os dados em streaming, permitindo o processamento eficiente de grandes volumes de dados.\n",
    "- **Pré-Treinamento em um Subconjunto dos Dados**: Realiza um pré-treinamento do modelo em um subconjunto dos dados para inicializar os pesos de forma eficaz.\n",
    "- **Fine-Tuning com o Dataset Completo**: Ajusta o modelo treinado previamente utilizando o dataset completo em streaming, refinando os pesos para melhorar a performance.\n",
    "- **Normalização dos Dados**: Aplica técnicas de normalização tanto nos dados de treinamento quanto no streaming, garantindo que as features estejam em uma escala adequada para o treinamento da RNA.\n",
    "- **Treinamento do Modelo**: Executa o processo de treinamento com callbacks para early stopping e checkpointing, salvando o melhor modelo baseado na perda de validação.\n",
    "- **Salvamento do Modelo Final**: Armazena o modelo treinado para uso futuro.\n",
    "\n",
    "### Bloco 5: Gráficos do Treinamento\n",
    "\n",
    "- **Visualização da Evolução da Perda (Loss)**: Plota gráficos comparativos das perdas de treinamento e validação durante as fases de pré-treinamento e fine-tuning, permitindo a análise do comportamento do modelo ao longo das épocas.\n",
    "\n",
    "### Bloco 6: Exemplos de Predições\n",
    "\n",
    "- **Comparação entre Predições e Valores Reais**: Realiza predições no conjunto de validação e exibe exemplos comparando os valores previstos com os valores reais, fornecendo uma avaliação qualitativa do desempenho do modelo.\n",
    "\n",
    "### Bloco 7: Execução do Notebook\n",
    "\n",
    "- **Execução da Função Principal**: Garante que a função `main()` seja executada quando o notebook for executado diretamente, iniciando todo o fluxo de treinamento e avaliação do modelo.\n",
    "\n",
    "## Considerações Finais\n",
    "\n",
    "Este notebook implementa um fluxo completo para treinar e avaliar um modelo de RNA utilizando dados de satélites. As principais etapas incluem:\n",
    "\n",
    "1. **Preparação dos Dados**: Carregamento eficiente, descoberta de features e normalização.\n",
    "2. **Construção do Modelo**: Definição de uma arquitetura robusta com blocos residuais.\n",
    "3. **Treinamento e Fine-Tuning**: Treinamento inicial em um subconjunto seguido de ajuste fino com o dataset completo.\n",
    "4. **Avaliação e Visualização**: Análise da evolução do treinamento e comparação das predições com os valores reais.\n",
    "\n",
    "Certifique-se de que todos os caminhos de arquivos estejam corretos e que as dependências estejam instaladas antes de executar o notebook. Este fluxo facilita a criação de modelos precisos para previsão de diferenças de latitude e longitude com base em dados de satélites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 01:26:37.292111: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-16 01:26:37.315176: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] GPUs detectadas: ['/physical_device:GPU:0']\n",
      "[INFO] Usando GPU: /physical_device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 01:26:38.427317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-16 01:26:38.431260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-16 01:26:38.431500: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de features: 645\n",
      "[INFO] Lendo dataset inteiro na RAM...\n",
      "[INFO] Formato X_np: (419878, 645)\n",
      "[INFO] Formato y_np: (419878, 2)\n",
      "[INFO] Normalizando dados...\n",
      "[INFO] Construindo modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 01:30:16.485194: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-16 01:30:16.487107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-16 01:30:16.491277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-16 01:30:16.584085: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-16 01:30:16.584297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-16 01:30:16.584310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-01-16 01:30:16.584455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-16 01:30:16.584479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13512 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 645)]                0         []                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 2048)                 1323008   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 2048)                 8192      ['dense[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 2048)                 0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 2048)                 4196352   ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 2048)                 8192      ['dense_1[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 2048)                 0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 2048)                 0         ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 2048)                 4196352   ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 2048)                 8192      ['dense_2[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 2048)                 0         ['dropout[0][0]',             \n",
      "                                                                     'batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 2048)                 0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 2048)                 4196352   ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 2048)                 8192      ['dense_3[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 2048)                 0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 2048)                 0         ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 2048)                 4196352   ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 2048)                 8192      ['dense_4[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 2048)                 0         ['activation_1[0][0]',        \n",
      "                                                                     'batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 2048)                 0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 2048)                 4196352   ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 2048)                 8192      ['dense_5[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 2048)                 0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 2048)                 0         ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 2048)                 4196352   ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 2048)                 8192      ['dense_6[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 2048)                 0         ['activation_3[0][0]',        \n",
      "                                                                     'batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 2048)                 0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 2048)                 4196352   ['activation_5[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 2048)                 8192      ['dense_7[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 2048)                 0         ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 2048)                 0         ['activation_6[0][0]']        \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 2048)                 4196352   ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 2048)                 8192      ['dense_8[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 2048)                 0         ['activation_5[0][0]',        \n",
      "                                                                     'batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 2048)                 0         ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 2048)                 4196352   ['activation_7[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 2048)                 8192      ['dense_9[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 2048)                 0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 2048)                 0         ['activation_8[0][0]']        \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 2048)                 4196352   ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 2048)                 8192      ['dense_10[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 2048)                 0         ['activation_7[0][0]',        \n",
      "                                                                     'batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_9 (Activation)   (None, 2048)                 0         ['add_4[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 2048)                 4196352   ['activation_9[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 2048)                 8192      ['dense_11[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_10 (Activation)  (None, 2048)                 0         ['batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 2048)                 0         ['activation_10[0][0]']       \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 2048)                 4196352   ['dropout_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 2048)                 8192      ['dense_12[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 2048)                 0         ['activation_9[0][0]',        \n",
      "                                                                     'batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_11 (Activation)  (None, 2048)                 0         ['add_5[0][0]']               \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 2048)                 4196352   ['activation_11[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 2048)                 8192      ['dense_13[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_12 (Activation)  (None, 2048)                 0         ['batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 2048)                 0         ['activation_12[0][0]']       \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, 2048)                 4196352   ['dropout_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 2048)                 8192      ['dense_14[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 2048)                 0         ['activation_11[0][0]',       \n",
      "                                                                     'batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_13 (Activation)  (None, 2048)                 0         ['add_6[0][0]']               \n",
      "                                                                                                  \n",
      " dense_15 (Dense)            (None, 2048)                 4196352   ['activation_13[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 2048)                 8192      ['dense_15[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_14 (Activation)  (None, 2048)                 0         ['batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 2048)                 0         ['activation_14[0][0]']       \n",
      "                                                                                                  \n",
      " dense_16 (Dense)            (None, 2048)                 4196352   ['dropout_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 2048)                 8192      ['dense_16[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 2048)                 0         ['activation_13[0][0]',       \n",
      "                                                                     'batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_15 (Activation)  (None, 2048)                 0         ['add_7[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)            (None, 1024)                 2098176   ['activation_15[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, 1024)                 4096      ['dense_17[0][0]']            \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)         (None, 1024)                 0         ['batch_normalization_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)            (None, 2)                    2050      ['dropout_9[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 70708226 (269.73 MB)\n",
      "Trainable params: 70636546 (269.46 MB)\n",
      "Non-trainable params: 71680 (280.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "=== Pré-treino offline no subset ===\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 01:30:20.420778: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2025-01-16 01:30:20.582183: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f07e85de910 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-01-16 01:30:20.582210: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4080, Compute Capability 8.9\n",
      "2025-01-16 01:30:20.585301: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-01-16 01:30:20.592801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2025-01-16 01:30:20.671812: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 0s - loss: 35.8889 - mae: 2.5322 - mse: 35.8172  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 23s 415ms/step - loss: 35.8889 - mae: 2.5322 - mse: 35.8172 - val_loss: 42.9877 - val_mae: 3.3559 - val_mse: 42.9160\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 16s 401ms/step - loss: 31.5708 - mae: 2.4043 - mse: 31.4991 - val_loss: 40.1322 - val_mae: 3.2786 - val_mse: 40.0604\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 17s 415ms/step - loss: 26.5300 - mae: 2.2506 - mse: 26.4582 - val_loss: 25.6782 - val_mae: 2.0783 - val_mse: 25.6065\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 18s 458ms/step - loss: 22.2503 - mae: 2.0731 - mse: 22.1785 - val_loss: 23.2814 - val_mae: 1.9245 - val_mse: 23.2097\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 17s 431ms/step - loss: 18.8617 - mae: 1.9197 - mse: 18.7900 - val_loss: 17.3429 - val_mae: 1.7890 - val_mse: 17.2711\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 18s 439ms/step - loss: 16.4682 - mae: 1.7699 - mse: 16.3965 - val_loss: 16.2002 - val_mae: 1.5568 - val_mse: 16.1285\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 17s 431ms/step - loss: 14.8586 - mae: 1.6532 - mse: 14.7869 - val_loss: 15.1245 - val_mae: 1.5157 - val_mse: 15.0527\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 18s 442ms/step - loss: 13.5558 - mae: 1.5441 - mse: 13.4840 - val_loss: 12.3619 - val_mae: 1.3936 - val_mse: 12.2901\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 18s 442ms/step - loss: 12.6140 - mae: 1.4413 - mse: 12.5422 - val_loss: 10.8607 - val_mae: 1.3409 - val_mse: 10.7890\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 16s 405ms/step - loss: 11.3053 - mae: 1.3456 - mse: 11.2336 - val_loss: 10.3804 - val_mae: 1.3007 - val_mse: 10.3086\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 17s 430ms/step - loss: 10.3777 - mae: 1.2526 - mse: 10.3059 - val_loss: 9.4816 - val_mae: 1.0610 - val_mse: 9.4098\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 16s 411ms/step - loss: 9.7099 - mae: 1.1826 - mse: 9.6381 - val_loss: 8.9206 - val_mae: 1.0308 - val_mse: 8.8488\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 18s 440ms/step - loss: 9.1286 - mae: 1.1162 - mse: 9.0568 - val_loss: 8.5516 - val_mae: 1.0143 - val_mse: 8.4798\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 17s 418ms/step - loss: 8.4649 - mae: 1.0679 - mse: 8.3931 - val_loss: 8.2681 - val_mae: 0.8357 - val_mse: 8.1963\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 18s 455ms/step - loss: 8.2853 - mae: 1.0174 - mse: 8.2135 - val_loss: 8.0951 - val_mae: 0.8700 - val_mse: 8.0234\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 17s 412ms/step - loss: 7.7902 - mae: 0.9765 - mse: 7.7184 - val_loss: 8.0161 - val_mae: 0.8295 - val_mse: 7.9443\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 18s 445ms/step - loss: 7.8982 - mae: 0.9590 - mse: 7.8264 - val_loss: 7.5390 - val_mae: 0.8422 - val_mse: 7.4672\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 18s 459ms/step - loss: 7.1165 - mae: 0.9194 - mse: 7.0447 - val_loss: 6.6455 - val_mae: 0.7407 - val_mse: 6.5737\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 8s 180ms/step - loss: 6.9522 - mae: 0.8956 - mse: 6.8804 - val_loss: 6.8557 - val_mae: 0.7256 - val_mse: 6.7839\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 7s 184ms/step - loss: 6.7621 - mae: 0.8780 - mse: 6.6903 - val_loss: 6.8571 - val_mae: 0.7376 - val_mse: 6.7854\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 16s 397ms/step - loss: 6.7205 - mae: 0.8609 - mse: 6.6487 - val_loss: 6.0872 - val_mae: 0.6923 - val_mse: 6.0155\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 18s 453ms/step - loss: 6.3332 - mae: 0.8446 - mse: 6.2614 - val_loss: 6.0562 - val_mae: 0.6768 - val_mse: 5.9844\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - 8s 182ms/step - loss: 6.1729 - mae: 0.8321 - mse: 6.1011 - val_loss: 6.4340 - val_mae: 0.7170 - val_mse: 6.3622\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 5.9814 - mae: 0.8232 - mse: 5.9096 - val_loss: 6.3082 - val_mae: 0.6971 - val_mse: 6.2364\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 5.6675 - mae: 0.7975 - mse: 5.5957 - val_loss: 6.0784 - val_mae: 0.6946 - val_mse: 6.0066\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 5.7172 - mae: 0.7957 - mse: 5.6454 - val_loss: 6.3208 - val_mae: 0.6607 - val_mse: 6.2490\n",
      "Epoch 27/1000\n",
      "40/40 [==============================] - 7s 182ms/step - loss: 5.8991 - mae: 0.7857 - mse: 5.8273 - val_loss: 7.9269 - val_mae: 0.7593 - val_mse: 7.8551\n",
      "Epoch 28/1000\n",
      "40/40 [==============================] - 17s 420ms/step - loss: 5.8434 - mae: 0.7860 - mse: 5.7716 - val_loss: 6.0372 - val_mae: 0.6696 - val_mse: 5.9654\n",
      "Epoch 29/1000\n",
      "40/40 [==============================] - 17s 426ms/step - loss: 5.2736 - mae: 0.7626 - mse: 5.2017 - val_loss: 5.3498 - val_mae: 0.6165 - val_mse: 5.2780\n",
      "Epoch 30/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 4.9720 - mae: 0.7442 - mse: 4.9002 - val_loss: 5.4997 - val_mae: 0.6334 - val_mse: 5.4279\n",
      "Epoch 31/1000\n",
      "40/40 [==============================] - 19s 470ms/step - loss: 5.0312 - mae: 0.7368 - mse: 4.9594 - val_loss: 5.2964 - val_mae: 0.6273 - val_mse: 5.2246\n",
      "Epoch 32/1000\n",
      "40/40 [==============================] - 17s 416ms/step - loss: 5.3661 - mae: 0.7387 - mse: 5.2943 - val_loss: 5.2126 - val_mae: 0.6040 - val_mse: 5.1407\n",
      "Epoch 33/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 5.4366 - mae: 0.7421 - mse: 5.3648 - val_loss: 6.2869 - val_mae: 0.6838 - val_mse: 6.2150\n",
      "Epoch 34/1000\n",
      "40/40 [==============================] - 18s 462ms/step - loss: 4.5769 - mae: 0.7131 - mse: 4.5051 - val_loss: 4.5904 - val_mae: 0.6076 - val_mse: 4.5185\n",
      "Epoch 35/1000\n",
      "40/40 [==============================] - 8s 182ms/step - loss: 4.5429 - mae: 0.7068 - mse: 4.4711 - val_loss: 4.7886 - val_mae: 0.6071 - val_mse: 4.7167\n",
      "Epoch 36/1000\n",
      "40/40 [==============================] - 7s 182ms/step - loss: 4.3066 - mae: 0.6927 - mse: 4.2347 - val_loss: 4.9835 - val_mae: 0.6066 - val_mse: 4.9117\n",
      "Epoch 37/1000\n",
      "40/40 [==============================] - 7s 182ms/step - loss: 4.6411 - mae: 0.6921 - mse: 4.5693 - val_loss: 4.9502 - val_mae: 0.5607 - val_mse: 4.8783\n",
      "Epoch 38/1000\n",
      "40/40 [==============================] - 17s 439ms/step - loss: 4.4610 - mae: 0.6883 - mse: 4.3892 - val_loss: 4.4932 - val_mae: 0.5642 - val_mse: 4.4213\n",
      "Epoch 39/1000\n",
      "40/40 [==============================] - 8s 182ms/step - loss: 4.2043 - mae: 0.6764 - mse: 4.1324 - val_loss: 4.5752 - val_mae: 0.5033 - val_mse: 4.5033\n",
      "Epoch 40/1000\n",
      "40/40 [==============================] - 17s 440ms/step - loss: 3.9001 - mae: 0.6607 - mse: 3.8282 - val_loss: 4.3527 - val_mae: 0.5418 - val_mse: 4.2808\n",
      "Epoch 41/1000\n",
      "40/40 [==============================] - 8s 182ms/step - loss: 4.0130 - mae: 0.6602 - mse: 3.9411 - val_loss: 4.5078 - val_mae: 0.5554 - val_mse: 4.4359\n",
      "Epoch 42/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 3.7967 - mae: 0.6513 - mse: 3.7248 - val_loss: 4.8802 - val_mae: 0.5371 - val_mse: 4.8082\n",
      "Epoch 43/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 3.9939 - mae: 0.6506 - mse: 3.9220 - val_loss: 5.4356 - val_mae: 0.5593 - val_mse: 5.3637\n",
      "Epoch 44/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 4.1668 - mae: 0.6560 - mse: 4.0949 - val_loss: 5.0498 - val_mae: 0.5972 - val_mse: 4.9778\n",
      "Epoch 45/1000\n",
      "40/40 [==============================] - 18s 466ms/step - loss: 3.8827 - mae: 0.6443 - mse: 3.8108 - val_loss: 4.1637 - val_mae: 0.4951 - val_mse: 4.0917\n",
      "Epoch 46/1000\n",
      "40/40 [==============================] - 8s 183ms/step - loss: 3.6535 - mae: 0.6338 - mse: 3.5816 - val_loss: 4.4654 - val_mae: 0.5143 - val_mse: 4.3934\n",
      "Epoch 47/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 3.5868 - mae: 0.6256 - mse: 3.5148 - val_loss: 4.5021 - val_mae: 0.5064 - val_mse: 4.4302\n",
      "Epoch 48/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 3.7353 - mae: 0.6279 - mse: 3.6633 - val_loss: 4.9341 - val_mae: 0.5266 - val_mse: 4.8621\n",
      "Epoch 49/1000\n",
      "40/40 [==============================] - 18s 445ms/step - loss: 3.7539 - mae: 0.6278 - mse: 3.6819 - val_loss: 4.0674 - val_mae: 0.5025 - val_mse: 3.9955\n",
      "Epoch 50/1000\n",
      "40/40 [==============================] - 17s 426ms/step - loss: 3.3826 - mae: 0.6151 - mse: 3.3106 - val_loss: 3.9497 - val_mae: 0.5260 - val_mse: 3.8777\n",
      "Epoch 51/1000\n",
      "40/40 [==============================] - 8s 185ms/step - loss: 3.5360 - mae: 0.6118 - mse: 3.4640 - val_loss: 4.6291 - val_mae: 0.4915 - val_mse: 4.5571\n",
      "Epoch 52/1000\n",
      "40/40 [==============================] - 7s 187ms/step - loss: 4.1462 - mae: 0.6220 - mse: 4.0742 - val_loss: 4.4907 - val_mae: 0.6019 - val_mse: 4.4187\n",
      "Epoch 53/1000\n",
      "40/40 [==============================] - 7s 185ms/step - loss: 3.7061 - mae: 0.6101 - mse: 3.6340 - val_loss: 3.9651 - val_mae: 0.4344 - val_mse: 3.8931\n",
      "Epoch 54/1000\n",
      "40/40 [==============================] - 7s 186ms/step - loss: 3.5821 - mae: 0.5975 - mse: 3.5100 - val_loss: 4.0002 - val_mae: 0.4575 - val_mse: 3.9282\n",
      "Epoch 55/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 4.1271 - mae: 0.6134 - mse: 4.0550 - val_loss: 4.3527 - val_mae: 0.4636 - val_mse: 4.2806\n",
      "Epoch 56/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 3.5153 - mae: 0.5989 - mse: 3.4432 - val_loss: 4.0413 - val_mae: 0.4739 - val_mse: 3.9692\n",
      "Epoch 57/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 3.1525 - mae: 0.5859 - mse: 3.0804 - val_loss: 4.2421 - val_mae: 0.4629 - val_mse: 4.1700\n",
      "Epoch 58/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 3.2312 - mae: 0.5805 - mse: 3.1591 - val_loss: 4.0915 - val_mae: 0.4879 - val_mse: 4.0194\n",
      "Epoch 59/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.9816 - mae: 0.5710 - mse: 2.9094 - val_loss: 4.9864 - val_mae: 0.4788 - val_mse: 4.9143\n",
      "Epoch 60/1000\n",
      "40/40 [==============================] - 19s 478ms/step - loss: 2.9681 - mae: 0.5634 - mse: 2.8959 - val_loss: 3.7759 - val_mae: 0.4359 - val_mse: 3.7037\n",
      "Epoch 61/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 3.0657 - mae: 0.5628 - mse: 2.9935 - val_loss: 3.9266 - val_mae: 0.4418 - val_mse: 3.8544\n",
      "Epoch 62/1000\n",
      "40/40 [==============================] - 18s 453ms/step - loss: 3.0854 - mae: 0.5646 - mse: 3.0132 - val_loss: 3.4098 - val_mae: 0.4118 - val_mse: 3.3376\n",
      "Epoch 63/1000\n",
      "40/40 [==============================] - 8s 183ms/step - loss: 2.9207 - mae: 0.5565 - mse: 2.8485 - val_loss: 3.6615 - val_mae: 0.4459 - val_mse: 3.5893\n",
      "Epoch 64/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.8020 - mae: 0.5523 - mse: 2.7298 - val_loss: 4.1467 - val_mae: 0.4467 - val_mse: 4.0746\n",
      "Epoch 65/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.7896 - mae: 0.5471 - mse: 2.7174 - val_loss: 3.9116 - val_mae: 0.4500 - val_mse: 3.8394\n",
      "Epoch 66/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.9139 - mae: 0.5500 - mse: 2.8417 - val_loss: 3.5222 - val_mae: 0.4747 - val_mse: 3.4500\n",
      "Epoch 67/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.8169 - mae: 0.5444 - mse: 2.7447 - val_loss: 3.4785 - val_mae: 0.4063 - val_mse: 3.4063\n",
      "Epoch 68/1000\n",
      "40/40 [==============================] - 17s 435ms/step - loss: 2.6565 - mae: 0.5377 - mse: 2.5843 - val_loss: 3.2490 - val_mae: 0.4174 - val_mse: 3.1767\n",
      "Epoch 69/1000\n",
      "40/40 [==============================] - 16s 392ms/step - loss: 2.8589 - mae: 0.5373 - mse: 2.7866 - val_loss: 3.0984 - val_mae: 0.4046 - val_mse: 3.0261\n",
      "Epoch 70/1000\n",
      "40/40 [==============================] - 8s 183ms/step - loss: 2.8380 - mae: 0.5369 - mse: 2.7657 - val_loss: 3.8837 - val_mae: 0.4387 - val_mse: 3.8114\n",
      "Epoch 71/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.4602 - mae: 0.5243 - mse: 2.3879 - val_loss: 3.3128 - val_mae: 0.3878 - val_mse: 3.2406\n",
      "Epoch 72/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.5619 - mae: 0.5191 - mse: 2.4897 - val_loss: 3.8302 - val_mae: 0.4282 - val_mse: 3.7579\n",
      "Epoch 73/1000\n",
      "40/40 [==============================] - 17s 438ms/step - loss: 2.6830 - mae: 0.5171 - mse: 2.6107 - val_loss: 3.0352 - val_mae: 0.3638 - val_mse: 2.9629\n",
      "Epoch 74/1000\n",
      "40/40 [==============================] - 8s 183ms/step - loss: 2.5103 - mae: 0.5165 - mse: 2.4380 - val_loss: 3.1682 - val_mae: 0.3879 - val_mse: 3.0960\n",
      "Epoch 75/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.2919 - mae: 0.5087 - mse: 2.2196 - val_loss: 3.4031 - val_mae: 0.3795 - val_mse: 3.3308\n",
      "Epoch 76/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.2645 - mae: 0.5082 - mse: 2.1922 - val_loss: 3.5205 - val_mae: 0.4156 - val_mse: 3.4482\n",
      "Epoch 77/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.4862 - mae: 0.5087 - mse: 2.4139 - val_loss: 3.4209 - val_mae: 0.4065 - val_mse: 3.3485\n",
      "Epoch 78/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.7261 - mae: 0.5176 - mse: 2.6538 - val_loss: 3.6504 - val_mae: 0.4033 - val_mse: 3.5781\n",
      "Epoch 79/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.4241 - mae: 0.5050 - mse: 2.3518 - val_loss: 3.4470 - val_mae: 0.4157 - val_mse: 3.3747\n",
      "Epoch 80/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.2690 - mae: 0.5005 - mse: 2.1966 - val_loss: 3.5595 - val_mae: 0.3932 - val_mse: 3.4871\n",
      "Epoch 81/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.3793 - mae: 0.4988 - mse: 2.3069 - val_loss: 3.5696 - val_mae: 0.4356 - val_mse: 3.4972\n",
      "Epoch 82/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.3942 - mae: 0.5005 - mse: 2.3218 - val_loss: 3.7361 - val_mae: 0.4129 - val_mse: 3.6637\n",
      "Epoch 83/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.3281 - mae: 0.4990 - mse: 2.2557 - val_loss: 3.1507 - val_mae: 0.3742 - val_mse: 3.0784\n",
      "Epoch 84/1000\n",
      "40/40 [==============================] - 16s 418ms/step - loss: 2.1616 - mae: 0.4848 - mse: 2.0892 - val_loss: 3.0142 - val_mae: 0.3677 - val_mse: 2.9418\n",
      "Epoch 85/1000\n",
      "40/40 [==============================] - 8s 183ms/step - loss: 2.2166 - mae: 0.4852 - mse: 2.1442 - val_loss: 3.0453 - val_mae: 0.3753 - val_mse: 2.9729\n",
      "Epoch 86/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.4239 - mae: 0.4919 - mse: 2.3514 - val_loss: 3.2319 - val_mae: 0.4103 - val_mse: 3.1594\n",
      "Epoch 87/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.3130 - mae: 0.4898 - mse: 2.2405 - val_loss: 3.5808 - val_mae: 0.4174 - val_mse: 3.5083\n",
      "Epoch 88/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.1158 - mae: 0.4783 - mse: 2.0433 - val_loss: 3.5739 - val_mae: 0.4487 - val_mse: 3.5015\n",
      "Epoch 89/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.7841 - mae: 0.4991 - mse: 2.7117 - val_loss: 3.0387 - val_mae: 0.4326 - val_mse: 2.9662\n",
      "Epoch 90/1000\n",
      "40/40 [==============================] - 17s 434ms/step - loss: 2.2987 - mae: 0.4901 - mse: 2.2262 - val_loss: 2.9174 - val_mae: 0.3845 - val_mse: 2.8449\n",
      "Epoch 91/1000\n",
      "40/40 [==============================] - 8s 183ms/step - loss: 2.1433 - mae: 0.4729 - mse: 2.0708 - val_loss: 3.2035 - val_mae: 0.3685 - val_mse: 3.1310\n",
      "Epoch 92/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.9382 - mae: 0.4624 - mse: 1.8657 - val_loss: 3.0860 - val_mae: 0.3847 - val_mse: 3.0135\n",
      "Epoch 93/1000\n",
      "40/40 [==============================] - 17s 442ms/step - loss: 1.8667 - mae: 0.4619 - mse: 1.7941 - val_loss: 2.8027 - val_mae: 0.3793 - val_mse: 2.7301\n",
      "Epoch 94/1000\n",
      "40/40 [==============================] - 8s 183ms/step - loss: 2.0757 - mae: 0.4633 - mse: 2.0032 - val_loss: 2.9812 - val_mae: 0.3793 - val_mse: 2.9087\n",
      "Epoch 95/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.9693 - mae: 0.4602 - mse: 1.8968 - val_loss: 3.0991 - val_mae: 0.3543 - val_mse: 3.0266\n",
      "Epoch 96/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.6052 - mae: 0.4792 - mse: 2.5326 - val_loss: 3.8046 - val_mae: 0.3915 - val_mse: 3.7320\n",
      "Epoch 97/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.6114 - mae: 0.4783 - mse: 2.5388 - val_loss: 4.2981 - val_mae: 0.3638 - val_mse: 4.2255\n",
      "Epoch 98/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.7007 - mae: 0.4752 - mse: 2.6281 - val_loss: 3.2404 - val_mae: 0.3643 - val_mse: 3.1677\n",
      "Epoch 99/1000\n",
      "40/40 [==============================] - 17s 442ms/step - loss: 2.0559 - mae: 0.4631 - mse: 1.9832 - val_loss: 2.7142 - val_mae: 0.3343 - val_mse: 2.6415\n",
      "Epoch 100/1000\n",
      "40/40 [==============================] - 19s 472ms/step - loss: 2.0571 - mae: 0.4613 - mse: 1.9845 - val_loss: 2.4493 - val_mae: 0.3112 - val_mse: 2.3766\n",
      "Epoch 101/1000\n",
      "40/40 [==============================] - 8s 183ms/step - loss: 1.9803 - mae: 0.4544 - mse: 1.9076 - val_loss: 2.6287 - val_mae: 0.3779 - val_mse: 2.5560\n",
      "Epoch 102/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.7905 - mae: 0.4449 - mse: 1.7179 - val_loss: 2.7936 - val_mae: 0.3346 - val_mse: 2.7209\n",
      "Epoch 103/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.0194 - mae: 0.4497 - mse: 1.9467 - val_loss: 3.1809 - val_mae: 0.3734 - val_mse: 3.1082\n",
      "Epoch 104/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.9564 - mae: 0.4436 - mse: 1.8837 - val_loss: 2.7136 - val_mae: 0.3110 - val_mse: 2.6409\n",
      "Epoch 105/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.7730 - mae: 0.4379 - mse: 1.7003 - val_loss: 2.7060 - val_mae: 0.3237 - val_mse: 2.6333\n",
      "Epoch 106/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.7615 - mae: 0.4371 - mse: 1.6888 - val_loss: 3.2575 - val_mae: 0.3657 - val_mse: 3.1848\n",
      "Epoch 107/1000\n",
      "40/40 [==============================] - 7s 182ms/step - loss: 1.9117 - mae: 0.4378 - mse: 1.8390 - val_loss: 3.4386 - val_mae: 0.3376 - val_mse: 3.3659\n",
      "Epoch 108/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.2885 - mae: 0.4418 - mse: 2.2157 - val_loss: 2.6560 - val_mae: 0.3332 - val_mse: 2.5832\n",
      "Epoch 109/1000\n",
      "40/40 [==============================] - 7s 182ms/step - loss: 1.9112 - mae: 0.4332 - mse: 1.8385 - val_loss: 2.8711 - val_mae: 0.3357 - val_mse: 2.7983\n",
      "Epoch 110/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.8083 - mae: 0.4263 - mse: 1.7356 - val_loss: 2.8439 - val_mae: 0.3727 - val_mse: 2.7712\n",
      "Epoch 111/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.7316 - mae: 0.4253 - mse: 1.6588 - val_loss: 2.7206 - val_mae: 0.3243 - val_mse: 2.6479\n",
      "Epoch 112/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.8004 - mae: 0.4282 - mse: 1.7276 - val_loss: 2.5867 - val_mae: 0.2766 - val_mse: 2.5139\n",
      "Epoch 113/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.7687 - mae: 0.4210 - mse: 1.6959 - val_loss: 3.1365 - val_mae: 0.3424 - val_mse: 3.0637\n",
      "Epoch 114/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.0155 - mae: 0.4275 - mse: 1.9427 - val_loss: 2.7488 - val_mae: 0.3323 - val_mse: 2.6760\n",
      "Epoch 115/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.8479 - mae: 0.4308 - mse: 1.7751 - val_loss: 2.7874 - val_mae: 0.3079 - val_mse: 2.7146\n",
      "Epoch 116/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.1670 - mae: 0.4373 - mse: 2.0942 - val_loss: 2.7741 - val_mae: 0.3261 - val_mse: 2.7012\n",
      "Epoch 117/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.7021 - mae: 0.4199 - mse: 1.6293 - val_loss: 2.7146 - val_mae: 0.3274 - val_mse: 2.6417\n",
      "Epoch 118/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.6599 - mae: 0.4105 - mse: 1.5871 - val_loss: 2.8513 - val_mae: 0.3245 - val_mse: 2.7784\n",
      "Epoch 119/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.7645 - mae: 0.4107 - mse: 1.6917 - val_loss: 2.4763 - val_mae: 0.3258 - val_mse: 2.4034\n",
      "Epoch 120/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.7618 - mae: 0.4124 - mse: 1.6889 - val_loss: 2.7659 - val_mae: 0.3336 - val_mse: 2.6930\n",
      "Epoch 121/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.8480 - mae: 0.4121 - mse: 1.7751 - val_loss: 2.6672 - val_mae: 0.3010 - val_mse: 2.5943\n",
      "Epoch 122/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.6029 - mae: 0.4072 - mse: 1.5300 - val_loss: 2.8437 - val_mae: 0.2935 - val_mse: 2.7708\n",
      "Epoch 123/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.8132 - mae: 0.4107 - mse: 1.7403 - val_loss: 3.0218 - val_mae: 0.3881 - val_mse: 2.9489\n",
      "Epoch 124/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.7052 - mae: 0.4057 - mse: 1.6322 - val_loss: 2.5859 - val_mae: 0.3434 - val_mse: 2.5130\n",
      "Epoch 125/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.6150 - mae: 0.4015 - mse: 1.5420 - val_loss: 2.6054 - val_mae: 0.2918 - val_mse: 2.5324\n",
      "Epoch 126/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.7429 - mae: 0.4035 - mse: 1.6700 - val_loss: 2.7650 - val_mae: 0.3029 - val_mse: 2.6920\n",
      "Epoch 127/1000\n",
      "40/40 [==============================] - 18s 452ms/step - loss: 1.6117 - mae: 0.3991 - mse: 1.5387 - val_loss: 2.3093 - val_mae: 0.2928 - val_mse: 2.2364\n",
      "Epoch 128/1000\n",
      "40/40 [==============================] - 8s 183ms/step - loss: 1.5550 - mae: 0.3970 - mse: 1.4820 - val_loss: 2.4689 - val_mae: 0.2808 - val_mse: 2.3959\n",
      "Epoch 129/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.7374 - mae: 0.4008 - mse: 1.6644 - val_loss: 2.8585 - val_mae: 0.3721 - val_mse: 2.7855\n",
      "Epoch 130/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.5008 - mae: 0.3936 - mse: 1.4278 - val_loss: 2.3849 - val_mae: 0.3397 - val_mse: 2.3118\n",
      "Epoch 131/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.4339 - mae: 0.3851 - mse: 1.3609 - val_loss: 2.7470 - val_mae: 0.3972 - val_mse: 2.6740\n",
      "Epoch 132/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.7297 - mae: 0.3965 - mse: 1.6567 - val_loss: 2.5177 - val_mae: 0.3063 - val_mse: 2.4447\n",
      "Epoch 133/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.4480 - mae: 0.3852 - mse: 1.3750 - val_loss: 2.3980 - val_mae: 0.3291 - val_mse: 2.3249\n",
      "Epoch 134/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.3553 - mae: 0.3793 - mse: 1.2822 - val_loss: 2.6685 - val_mae: 0.3386 - val_mse: 2.5954\n",
      "Epoch 135/1000\n",
      "40/40 [==============================] - 18s 449ms/step - loss: 1.4939 - mae: 0.3822 - mse: 1.4208 - val_loss: 2.1766 - val_mae: 0.2755 - val_mse: 2.1035\n",
      "Epoch 136/1000\n",
      "40/40 [==============================] - 8s 183ms/step - loss: 1.4089 - mae: 0.3774 - mse: 1.3358 - val_loss: 2.5658 - val_mae: 0.2968 - val_mse: 2.4927\n",
      "Epoch 137/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.9378 - mae: 0.3870 - mse: 1.8647 - val_loss: 3.4668 - val_mae: 0.2903 - val_mse: 3.3937\n",
      "Epoch 138/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.5355 - mae: 0.4200 - mse: 2.4624 - val_loss: 3.7842 - val_mae: 0.4332 - val_mse: 3.7111\n",
      "Epoch 139/1000\n",
      "40/40 [==============================] - 7s 182ms/step - loss: 2.4467 - mae: 0.4239 - mse: 2.3736 - val_loss: 3.4954 - val_mae: 0.4001 - val_mse: 3.4222\n",
      "Epoch 140/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.3070 - mae: 0.4216 - mse: 2.2337 - val_loss: 3.4816 - val_mae: 0.3310 - val_mse: 3.4084\n",
      "Epoch 141/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.9873 - mae: 0.3997 - mse: 1.9140 - val_loss: 2.8191 - val_mae: 0.3356 - val_mse: 2.7458\n",
      "Epoch 142/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.6503 - mae: 0.3936 - mse: 1.5770 - val_loss: 2.8011 - val_mae: 0.3184 - val_mse: 2.7278\n",
      "Epoch 143/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.9980 - mae: 0.3986 - mse: 1.9246 - val_loss: 2.5631 - val_mae: 0.3265 - val_mse: 2.4898\n",
      "Epoch 144/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.8860 - mae: 0.3927 - mse: 1.8127 - val_loss: 2.7561 - val_mae: 0.3207 - val_mse: 2.6828\n",
      "Epoch 145/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.4679 - mae: 0.4224 - mse: 2.3946 - val_loss: 3.5721 - val_mae: 0.3776 - val_mse: 3.4987\n",
      "Epoch 146/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.0098 - mae: 0.4039 - mse: 1.9364 - val_loss: 2.3649 - val_mae: 0.3112 - val_mse: 2.2915\n",
      "Epoch 147/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.8512 - mae: 0.3911 - mse: 1.7777 - val_loss: 2.7321 - val_mae: 0.3128 - val_mse: 2.6587\n",
      "Epoch 148/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.8235 - mae: 0.3892 - mse: 1.7501 - val_loss: 2.5898 - val_mae: 0.3376 - val_mse: 2.5163\n",
      "Epoch 149/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.4276 - mae: 0.3760 - mse: 1.3541 - val_loss: 2.6831 - val_mae: 0.2584 - val_mse: 2.6096\n",
      "Epoch 150/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.5150 - mae: 0.3762 - mse: 1.4415 - val_loss: 2.4044 - val_mae: 0.2788 - val_mse: 2.3309\n",
      "Epoch 151/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.6792 - mae: 0.3798 - mse: 1.6057 - val_loss: 2.4780 - val_mae: 0.3801 - val_mse: 2.4045\n",
      "Epoch 152/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.6735 - mae: 0.3729 - mse: 1.6000 - val_loss: 2.6427 - val_mae: 0.2742 - val_mse: 2.5691\n",
      "Epoch 153/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.4650 - mae: 0.3655 - mse: 1.3915 - val_loss: 2.8255 - val_mae: 0.2702 - val_mse: 2.7519\n",
      "Epoch 154/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.3842 - mae: 0.3643 - mse: 1.3107 - val_loss: 2.3547 - val_mae: 0.2969 - val_mse: 2.2811\n",
      "Epoch 155/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.3330 - mae: 0.3605 - mse: 1.2595 - val_loss: 2.6934 - val_mae: 0.2896 - val_mse: 2.6199\n",
      "Epoch 156/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.3962 - mae: 0.3615 - mse: 1.3226 - val_loss: 2.5667 - val_mae: 0.2713 - val_mse: 2.4931\n",
      "Epoch 157/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.3258 - mae: 0.3600 - mse: 1.2522 - val_loss: 2.3048 - val_mae: 0.2713 - val_mse: 2.2312\n",
      "Epoch 158/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2254 - mae: 0.3543 - mse: 1.1518 - val_loss: 2.2263 - val_mae: 0.3081 - val_mse: 2.1527\n",
      "Epoch 159/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2674 - mae: 0.3542 - mse: 1.1938 - val_loss: 2.3205 - val_mae: 0.2854 - val_mse: 2.2469\n",
      "Epoch 160/1000\n",
      "40/40 [==============================] - 16s 411ms/step - loss: 1.2619 - mae: 0.3502 - mse: 1.1883 - val_loss: 2.1457 - val_mae: 0.2561 - val_mse: 2.0721\n",
      "Epoch 161/1000\n",
      "40/40 [==============================] - 8s 183ms/step - loss: 1.1443 - mae: 0.3468 - mse: 1.0707 - val_loss: 2.2693 - val_mae: 0.2618 - val_mse: 2.1956\n",
      "Epoch 162/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0864 - mae: 0.3427 - mse: 1.0128 - val_loss: 2.2870 - val_mae: 0.2482 - val_mse: 2.2134\n",
      "Epoch 163/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.5688 - mae: 0.3567 - mse: 1.4952 - val_loss: 2.4822 - val_mae: 0.2696 - val_mse: 2.4086\n",
      "Epoch 164/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.4616 - mae: 0.3537 - mse: 1.3880 - val_loss: 2.3236 - val_mae: 0.2986 - val_mse: 2.2500\n",
      "Epoch 165/1000\n",
      "40/40 [==============================] - 16s 415ms/step - loss: 1.2131 - mae: 0.3450 - mse: 1.1395 - val_loss: 2.0824 - val_mae: 0.2457 - val_mse: 2.0087\n",
      "Epoch 166/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1061 - mae: 0.3365 - mse: 1.0324 - val_loss: 2.4198 - val_mae: 0.2594 - val_mse: 2.3462\n",
      "Epoch 167/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.7096 - mae: 0.3552 - mse: 1.6359 - val_loss: 2.1687 - val_mae: 0.2907 - val_mse: 2.0950\n",
      "Epoch 168/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.3603 - mae: 0.3592 - mse: 1.2866 - val_loss: 2.4545 - val_mae: 0.3065 - val_mse: 2.3808\n",
      "Epoch 169/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.3572 - mae: 0.3500 - mse: 1.2835 - val_loss: 2.7417 - val_mae: 0.2725 - val_mse: 2.6680\n",
      "Epoch 170/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2969 - mae: 0.3501 - mse: 1.2232 - val_loss: 2.4470 - val_mae: 0.3524 - val_mse: 2.3732\n",
      "Epoch 171/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2350 - mae: 0.3431 - mse: 1.1613 - val_loss: 2.4111 - val_mae: 0.2426 - val_mse: 2.3374\n",
      "Epoch 172/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2656 - mae: 0.3451 - mse: 1.1918 - val_loss: 2.3486 - val_mae: 0.2707 - val_mse: 2.2748\n",
      "Epoch 173/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1219 - mae: 0.3341 - mse: 1.0481 - val_loss: 2.2939 - val_mae: 0.2556 - val_mse: 2.2202\n",
      "Epoch 174/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0518 - mae: 0.3329 - mse: 0.9780 - val_loss: 2.2999 - val_mae: 0.3393 - val_mse: 2.2261\n",
      "Epoch 175/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1881 - mae: 0.3379 - mse: 1.1143 - val_loss: 2.4846 - val_mae: 0.2409 - val_mse: 2.4108\n",
      "Epoch 176/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2901 - mae: 0.3392 - mse: 1.2164 - val_loss: 2.3972 - val_mae: 0.2799 - val_mse: 2.3235\n",
      "Epoch 177/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1953 - mae: 0.3381 - mse: 1.1215 - val_loss: 2.4665 - val_mae: 0.2903 - val_mse: 2.3928\n",
      "Epoch 178/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.3123 - mae: 0.3472 - mse: 1.2386 - val_loss: 2.4510 - val_mae: 0.2321 - val_mse: 2.3772\n",
      "Epoch 179/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.4252 - mae: 0.3415 - mse: 1.3514 - val_loss: 2.5864 - val_mae: 0.3059 - val_mse: 2.5125\n",
      "Epoch 180/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.3245 - mae: 0.3373 - mse: 1.2506 - val_loss: 2.7173 - val_mae: 0.2602 - val_mse: 2.6435\n",
      "Epoch 181/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2267 - mae: 0.3341 - mse: 1.1529 - val_loss: 2.2692 - val_mae: 0.3356 - val_mse: 2.1953\n",
      "Epoch 182/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1376 - mae: 0.3315 - mse: 1.0638 - val_loss: 2.4909 - val_mae: 0.2646 - val_mse: 2.4171\n",
      "Epoch 183/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.3766 - mae: 0.3361 - mse: 1.3027 - val_loss: 2.3107 - val_mae: 0.3081 - val_mse: 2.2369\n",
      "Epoch 184/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1146 - mae: 0.3316 - mse: 1.0407 - val_loss: 2.1707 - val_mae: 0.2510 - val_mse: 2.0968\n",
      "Epoch 185/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0725 - mae: 0.3212 - mse: 0.9986 - val_loss: 2.2045 - val_mae: 0.2481 - val_mse: 2.1306\n",
      "Epoch 186/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0934 - mae: 0.3248 - mse: 1.0195 - val_loss: 2.4300 - val_mae: 0.2769 - val_mse: 2.3561\n",
      "Epoch 187/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0890 - mae: 0.3216 - mse: 1.0151 - val_loss: 2.3163 - val_mae: 0.2604 - val_mse: 2.2424\n",
      "Epoch 188/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1262 - mae: 0.3220 - mse: 1.0523 - val_loss: 2.4422 - val_mae: 0.2557 - val_mse: 2.3683\n",
      "Epoch 189/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0326 - mae: 0.3204 - mse: 0.9587 - val_loss: 2.1909 - val_mae: 0.2740 - val_mse: 2.1170\n",
      "Epoch 190/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0097 - mae: 0.3223 - mse: 0.9358 - val_loss: 2.1990 - val_mae: 0.2725 - val_mse: 2.1250\n",
      "Epoch 191/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1428 - mae: 0.3263 - mse: 1.0689 - val_loss: 2.2448 - val_mae: 0.2608 - val_mse: 2.1709\n",
      "Epoch 192/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2300 - mae: 0.3237 - mse: 1.1561 - val_loss: 2.3378 - val_mae: 0.2814 - val_mse: 2.2639\n",
      "Epoch 193/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2668 - mae: 0.3306 - mse: 1.1929 - val_loss: 2.5040 - val_mae: 0.2683 - val_mse: 2.4300\n",
      "Epoch 194/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.5236 - mae: 0.3328 - mse: 1.4496 - val_loss: 2.3421 - val_mae: 0.2466 - val_mse: 2.2681\n",
      "Epoch 195/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2889 - mae: 0.3337 - mse: 1.2149 - val_loss: 2.1578 - val_mae: 0.2544 - val_mse: 2.0838\n",
      "Epoch 196/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2395 - mae: 0.3317 - mse: 1.1655 - val_loss: 2.5217 - val_mae: 0.3469 - val_mse: 2.4477\n",
      "Epoch 197/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.3625 - mae: 0.3342 - mse: 1.2884 - val_loss: 2.4905 - val_mae: 0.2922 - val_mse: 2.4165\n",
      "Epoch 198/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0963 - mae: 0.3304 - mse: 1.0222 - val_loss: 2.2687 - val_mae: 0.2902 - val_mse: 2.1946\n",
      "Epoch 199/1000\n",
      "40/40 [==============================] - 18s 451ms/step - loss: 1.0099 - mae: 0.3185 - mse: 0.9358 - val_loss: 2.0794 - val_mae: 0.2487 - val_mse: 2.0053\n",
      "Epoch 200/1000\n",
      "40/40 [==============================] - 8s 183ms/step - loss: 1.1288 - mae: 0.3225 - mse: 1.0547 - val_loss: 2.6779 - val_mae: 0.2525 - val_mse: 2.6038\n",
      "Epoch 201/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.4221 - mae: 0.3283 - mse: 1.3480 - val_loss: 2.3428 - val_mae: 0.2837 - val_mse: 2.2687\n",
      "Epoch 202/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.3891 - mae: 0.3337 - mse: 1.3150 - val_loss: 2.4787 - val_mae: 0.3050 - val_mse: 2.4046\n",
      "Epoch 203/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1273 - mae: 0.3202 - mse: 1.0532 - val_loss: 2.4575 - val_mae: 0.2530 - val_mse: 2.3833\n",
      "Epoch 204/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1659 - mae: 0.3183 - mse: 1.0918 - val_loss: 2.4623 - val_mae: 0.2509 - val_mse: 2.3881\n",
      "Epoch 205/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0762 - mae: 0.3153 - mse: 1.0021 - val_loss: 2.2375 - val_mae: 0.2618 - val_mse: 2.1633\n",
      "Epoch 206/1000\n",
      "40/40 [==============================] - 15s 371ms/step - loss: 1.1644 - mae: 0.3090 - mse: 1.0903 - val_loss: 2.0314 - val_mae: 0.2526 - val_mse: 1.9572\n",
      "Epoch 207/1000\n",
      "40/40 [==============================] - 8s 183ms/step - loss: 1.1303 - mae: 0.3104 - mse: 1.0561 - val_loss: 2.4315 - val_mae: 0.2325 - val_mse: 2.3573\n",
      "Epoch 208/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1797 - mae: 0.3173 - mse: 1.1055 - val_loss: 2.6908 - val_mae: 0.2336 - val_mse: 2.6166\n",
      "Epoch 209/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2252 - mae: 0.3150 - mse: 1.1510 - val_loss: 3.0678 - val_mae: 0.2334 - val_mse: 2.9936\n",
      "Epoch 210/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1399 - mae: 0.3147 - mse: 1.0657 - val_loss: 2.7852 - val_mae: 0.2181 - val_mse: 2.7110\n",
      "Epoch 211/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.5118 - mae: 0.3276 - mse: 1.4376 - val_loss: 2.4966 - val_mae: 0.2964 - val_mse: 2.4223\n",
      "Epoch 212/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2415 - mae: 0.3161 - mse: 1.1673 - val_loss: 2.6541 - val_mae: 0.2987 - val_mse: 2.5799\n",
      "Epoch 213/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2215 - mae: 0.3156 - mse: 1.1472 - val_loss: 2.4561 - val_mae: 0.2672 - val_mse: 2.3818\n",
      "Epoch 214/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0113 - mae: 0.3085 - mse: 0.9370 - val_loss: 2.4228 - val_mae: 0.2040 - val_mse: 2.3485\n",
      "Epoch 215/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9937 - mae: 0.3081 - mse: 0.9194 - val_loss: 2.3447 - val_mae: 0.2510 - val_mse: 2.2703\n",
      "Epoch 216/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0027 - mae: 0.3035 - mse: 0.9284 - val_loss: 2.0392 - val_mae: 0.2161 - val_mse: 1.9649\n",
      "Epoch 217/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9743 - mae: 0.3025 - mse: 0.8999 - val_loss: 2.1836 - val_mae: 0.2468 - val_mse: 2.1093\n",
      "Epoch 218/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.3435 - mae: 0.3153 - mse: 1.2692 - val_loss: 2.2272 - val_mae: 0.2376 - val_mse: 2.1529\n",
      "Epoch 219/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1010 - mae: 0.3103 - mse: 1.0266 - val_loss: 2.0956 - val_mae: 0.2318 - val_mse: 2.0212\n",
      "Epoch 220/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2129 - mae: 0.3122 - mse: 1.1386 - val_loss: 2.9436 - val_mae: 0.2516 - val_mse: 2.8692\n",
      "Epoch 221/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.5057 - mae: 0.3318 - mse: 1.4313 - val_loss: 2.7343 - val_mae: 0.3491 - val_mse: 2.6599\n",
      "Epoch 222/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.3004 - mae: 0.3260 - mse: 1.2260 - val_loss: 2.6744 - val_mae: 0.2922 - val_mse: 2.5999\n",
      "Epoch 223/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0783 - mae: 0.3152 - mse: 1.0038 - val_loss: 2.4585 - val_mae: 0.3279 - val_mse: 2.3840\n",
      "Epoch 224/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1571 - mae: 0.3121 - mse: 1.0826 - val_loss: 2.1755 - val_mae: 0.2595 - val_mse: 2.1010\n",
      "Epoch 225/1000\n",
      "40/40 [==============================] - 18s 456ms/step - loss: 0.9423 - mae: 0.3048 - mse: 0.8678 - val_loss: 1.8700 - val_mae: 0.2145 - val_mse: 1.7955\n",
      "Epoch 226/1000\n",
      "40/40 [==============================] - 8s 183ms/step - loss: 1.3697 - mae: 0.3135 - mse: 1.2952 - val_loss: 2.6213 - val_mae: 0.2937 - val_mse: 2.5468\n",
      "Epoch 227/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.7160 - mae: 0.3336 - mse: 1.6414 - val_loss: 2.3275 - val_mae: 0.2770 - val_mse: 2.2529\n",
      "Epoch 228/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1771 - mae: 0.3149 - mse: 1.1025 - val_loss: 2.0359 - val_mae: 0.2378 - val_mse: 1.9613\n",
      "Epoch 229/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1486 - mae: 0.3075 - mse: 1.0740 - val_loss: 2.3350 - val_mae: 0.2614 - val_mse: 2.2603\n",
      "Epoch 230/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1584 - mae: 0.3072 - mse: 1.0838 - val_loss: 2.3059 - val_mae: 0.2263 - val_mse: 2.2312\n",
      "Epoch 231/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0761 - mae: 0.3024 - mse: 1.0014 - val_loss: 2.2505 - val_mae: 0.2374 - val_mse: 2.1759\n",
      "Epoch 232/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0693 - mae: 0.2997 - mse: 0.9946 - val_loss: 2.4368 - val_mae: 0.2389 - val_mse: 2.3621\n",
      "Epoch 233/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0050 - mae: 0.2991 - mse: 0.9303 - val_loss: 2.2333 - val_mae: 0.2768 - val_mse: 2.1586\n",
      "Epoch 234/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2324 - mae: 0.3107 - mse: 1.1577 - val_loss: 2.4607 - val_mae: 0.2228 - val_mse: 2.3859\n",
      "Epoch 235/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.3784 - mae: 0.3116 - mse: 1.3037 - val_loss: 2.5604 - val_mae: 0.2466 - val_mse: 2.4857\n",
      "Epoch 236/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.3085 - mae: 0.3110 - mse: 1.2337 - val_loss: 2.4487 - val_mae: 0.2304 - val_mse: 2.3740\n",
      "Epoch 237/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0698 - mae: 0.3045 - mse: 0.9950 - val_loss: 2.5375 - val_mae: 0.3079 - val_mse: 2.4627\n",
      "Epoch 238/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.3936 - mae: 0.3133 - mse: 1.3188 - val_loss: 2.5391 - val_mae: 0.2203 - val_mse: 2.4643\n",
      "Epoch 239/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.4181 - mae: 0.3159 - mse: 1.3433 - val_loss: 2.7385 - val_mae: 0.2367 - val_mse: 2.6636\n",
      "Epoch 240/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1160 - mae: 0.3080 - mse: 1.0411 - val_loss: 2.4188 - val_mae: 0.2300 - val_mse: 2.3439\n",
      "Epoch 241/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2059 - mae: 0.3053 - mse: 1.1310 - val_loss: 2.3666 - val_mae: 0.2074 - val_mse: 2.2917\n",
      "Epoch 242/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1367 - mae: 0.3053 - mse: 1.0618 - val_loss: 2.4875 - val_mae: 0.2749 - val_mse: 2.4126\n",
      "Epoch 243/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.3842 - mae: 0.3150 - mse: 1.3093 - val_loss: 2.1723 - val_mae: 0.2792 - val_mse: 2.0974\n",
      "Epoch 244/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1090 - mae: 0.3088 - mse: 1.0340 - val_loss: 1.9667 - val_mae: 0.2324 - val_mse: 1.8918\n",
      "Epoch 245/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1380 - mae: 0.2995 - mse: 1.0631 - val_loss: 2.2045 - val_mae: 0.2290 - val_mse: 2.1296\n",
      "Epoch 246/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9643 - mae: 0.2976 - mse: 0.8893 - val_loss: 2.1542 - val_mae: 0.2574 - val_mse: 2.0792\n",
      "Epoch 247/1000\n",
      "40/40 [==============================] - 18s 447ms/step - loss: 0.8854 - mae: 0.2903 - mse: 0.8104 - val_loss: 1.8445 - val_mae: 0.2242 - val_mse: 1.7695\n",
      "Epoch 248/1000\n",
      "40/40 [==============================] - 8s 183ms/step - loss: 1.1447 - mae: 0.2963 - mse: 1.0697 - val_loss: 1.8631 - val_mae: 0.1870 - val_mse: 1.7881\n",
      "Epoch 249/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9109 - mae: 0.2916 - mse: 0.8359 - val_loss: 1.8934 - val_mae: 0.1886 - val_mse: 1.8184\n",
      "Epoch 250/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9763 - mae: 0.2973 - mse: 0.9013 - val_loss: 1.9155 - val_mae: 0.2128 - val_mse: 1.8405\n",
      "Epoch 251/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1714 - mae: 0.2971 - mse: 1.0964 - val_loss: 2.2288 - val_mae: 0.2548 - val_mse: 2.1537\n",
      "Epoch 252/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1277 - mae: 0.2990 - mse: 1.0527 - val_loss: 2.4255 - val_mae: 0.2658 - val_mse: 2.3504\n",
      "Epoch 253/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9018 - mae: 0.2890 - mse: 0.8267 - val_loss: 2.2096 - val_mae: 0.2360 - val_mse: 2.1345\n",
      "Epoch 254/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8579 - mae: 0.2843 - mse: 0.7828 - val_loss: 2.2972 - val_mae: 0.2959 - val_mse: 2.2221\n",
      "Epoch 255/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0696 - mae: 0.2942 - mse: 0.9945 - val_loss: 2.3958 - val_mae: 0.3144 - val_mse: 2.3207\n",
      "Epoch 256/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0879 - mae: 0.2949 - mse: 1.0128 - val_loss: 2.3397 - val_mae: 0.2787 - val_mse: 2.2646\n",
      "Epoch 257/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.7778 - mae: 0.3230 - mse: 1.7026 - val_loss: 2.8324 - val_mae: 0.5234 - val_mse: 2.7572\n",
      "Epoch 258/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.6086 - mae: 0.3209 - mse: 1.5334 - val_loss: 2.0544 - val_mae: 0.2511 - val_mse: 1.9792\n",
      "Epoch 259/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1833 - mae: 0.3081 - mse: 1.1080 - val_loss: 2.4362 - val_mae: 0.3030 - val_mse: 2.3609\n",
      "Epoch 260/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.3561 - mae: 0.3141 - mse: 1.2808 - val_loss: 2.3133 - val_mae: 0.2093 - val_mse: 2.2380\n",
      "Epoch 261/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9091 - mae: 0.2919 - mse: 0.8338 - val_loss: 2.1501 - val_mae: 0.2024 - val_mse: 2.0748\n",
      "Epoch 262/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9043 - mae: 0.2844 - mse: 0.8290 - val_loss: 2.8214 - val_mae: 0.2540 - val_mse: 2.7461\n",
      "Epoch 263/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1842 - mae: 0.2991 - mse: 1.1089 - val_loss: 2.2098 - val_mae: 0.2067 - val_mse: 2.1345\n",
      "Epoch 264/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0126 - mae: 0.2938 - mse: 0.9372 - val_loss: 2.5067 - val_mae: 0.2504 - val_mse: 2.4313\n",
      "Epoch 265/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8381 - mae: 0.2877 - mse: 0.7628 - val_loss: 2.2699 - val_mae: 0.2702 - val_mse: 2.1946\n",
      "Epoch 266/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8525 - mae: 0.2817 - mse: 0.7772 - val_loss: 1.9377 - val_mae: 0.1781 - val_mse: 1.8624\n",
      "Epoch 267/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9076 - mae: 0.2797 - mse: 0.8322 - val_loss: 2.0942 - val_mae: 0.2793 - val_mse: 2.0188\n",
      "Epoch 268/1000\n",
      "40/40 [==============================] - 18s 447ms/step - loss: 1.0019 - mae: 0.2856 - mse: 0.9266 - val_loss: 1.7746 - val_mae: 0.2077 - val_mse: 1.6992\n",
      "Epoch 269/1000\n",
      "40/40 [==============================] - 17s 435ms/step - loss: 0.8846 - mae: 0.2857 - mse: 0.8092 - val_loss: 1.7368 - val_mae: 0.2107 - val_mse: 1.6614\n",
      "Epoch 270/1000\n",
      "40/40 [==============================] - 8s 183ms/step - loss: 1.2059 - mae: 0.2916 - mse: 1.1305 - val_loss: 2.7309 - val_mae: 0.2877 - val_mse: 2.6554\n",
      "Epoch 271/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.4614 - mae: 0.3130 - mse: 1.3860 - val_loss: 1.9404 - val_mae: 0.2259 - val_mse: 1.8650\n",
      "Epoch 272/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9010 - mae: 0.2920 - mse: 0.8256 - val_loss: 1.8769 - val_mae: 0.2438 - val_mse: 1.8014\n",
      "Epoch 273/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0037 - mae: 0.2917 - mse: 0.9282 - val_loss: 2.2813 - val_mae: 0.2554 - val_mse: 2.2058\n",
      "Epoch 274/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.3376 - mae: 0.2992 - mse: 1.2621 - val_loss: 2.2571 - val_mae: 0.2146 - val_mse: 2.1817\n",
      "Epoch 275/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9757 - mae: 0.2877 - mse: 0.9003 - val_loss: 1.8138 - val_mae: 0.2203 - val_mse: 1.7383\n",
      "Epoch 276/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0534 - mae: 0.2852 - mse: 0.9779 - val_loss: 2.1808 - val_mae: 0.2273 - val_mse: 2.1053\n",
      "Epoch 277/1000\n",
      "40/40 [==============================] - 7s 182ms/step - loss: 0.9506 - mae: 0.2846 - mse: 0.8750 - val_loss: 1.8358 - val_mae: 0.2777 - val_mse: 1.7603\n",
      "Epoch 278/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7571 - mae: 0.2751 - mse: 0.6816 - val_loss: 1.8422 - val_mae: 0.1965 - val_mse: 1.7667\n",
      "Epoch 279/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8627 - mae: 0.2788 - mse: 0.7871 - val_loss: 1.7696 - val_mae: 0.2654 - val_mse: 1.6941\n",
      "Epoch 280/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9079 - mae: 0.2833 - mse: 0.8324 - val_loss: 1.8352 - val_mae: 0.2337 - val_mse: 1.7596\n",
      "Epoch 281/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8279 - mae: 0.2782 - mse: 0.7524 - val_loss: 2.0270 - val_mae: 0.2256 - val_mse: 1.9514\n",
      "Epoch 282/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0105 - mae: 0.2812 - mse: 0.9349 - val_loss: 2.4004 - val_mae: 0.2452 - val_mse: 2.3248\n",
      "Epoch 283/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1817 - mae: 0.2895 - mse: 1.1061 - val_loss: 2.9111 - val_mae: 0.2675 - val_mse: 2.8355\n",
      "Epoch 284/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2626 - mae: 0.2953 - mse: 1.1870 - val_loss: 2.1640 - val_mae: 0.2186 - val_mse: 2.0884\n",
      "Epoch 285/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0122 - mae: 0.2837 - mse: 0.9366 - val_loss: 2.3732 - val_mae: 0.2308 - val_mse: 2.2976\n",
      "Epoch 286/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0262 - mae: 0.2908 - mse: 0.9505 - val_loss: 2.1815 - val_mae: 0.1835 - val_mse: 2.1059\n",
      "Epoch 287/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0261 - mae: 0.2889 - mse: 0.9505 - val_loss: 2.2681 - val_mae: 0.2144 - val_mse: 2.1924\n",
      "Epoch 288/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0901 - mae: 0.2857 - mse: 1.0144 - val_loss: 2.1497 - val_mae: 0.1878 - val_mse: 2.0741\n",
      "Epoch 289/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8127 - mae: 0.2795 - mse: 0.7370 - val_loss: 2.1198 - val_mae: 0.1786 - val_mse: 2.0441\n",
      "Epoch 290/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8083 - mae: 0.2720 - mse: 0.7326 - val_loss: 2.0145 - val_mae: 0.2227 - val_mse: 1.9388\n",
      "Epoch 291/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7417 - mae: 0.2720 - mse: 0.6660 - val_loss: 2.2741 - val_mae: 0.1754 - val_mse: 2.1984\n",
      "Epoch 292/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7709 - mae: 0.2669 - mse: 0.6952 - val_loss: 1.9491 - val_mae: 0.2772 - val_mse: 1.8734\n",
      "Epoch 293/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8013 - mae: 0.2723 - mse: 0.7256 - val_loss: 1.9270 - val_mae: 0.1673 - val_mse: 1.8513\n",
      "Epoch 294/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8351 - mae: 0.2691 - mse: 0.7594 - val_loss: 2.1741 - val_mae: 0.2094 - val_mse: 2.0983\n",
      "Epoch 295/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9061 - mae: 0.2740 - mse: 0.8304 - val_loss: 2.2026 - val_mae: 0.2130 - val_mse: 2.1269\n",
      "Epoch 296/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9904 - mae: 0.2794 - mse: 0.9146 - val_loss: 2.5099 - val_mae: 0.3113 - val_mse: 2.4342\n",
      "Epoch 297/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8572 - mae: 0.2725 - mse: 0.7815 - val_loss: 2.0951 - val_mae: 0.1779 - val_mse: 2.0194\n",
      "Epoch 298/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7932 - mae: 0.2732 - mse: 0.7174 - val_loss: 2.3269 - val_mae: 0.2097 - val_mse: 2.2512\n",
      "Epoch 299/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7623 - mae: 0.2695 - mse: 0.6865 - val_loss: 2.0278 - val_mae: 0.1648 - val_mse: 1.9520\n",
      "Epoch 300/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0229 - mae: 0.2678 - mse: 0.9472 - val_loss: 2.3349 - val_mae: 0.2230 - val_mse: 2.2592\n",
      "Epoch 301/1000\n",
      "40/40 [==============================] - 18s 455ms/step - loss: 1.0600 - mae: 0.2792 - mse: 0.9842 - val_loss: 1.6933 - val_mae: 0.2139 - val_mse: 1.6175\n",
      "Epoch 302/1000\n",
      "40/40 [==============================] - 8s 183ms/step - loss: 0.8442 - mae: 0.2718 - mse: 0.7684 - val_loss: 1.9223 - val_mae: 0.2059 - val_mse: 1.8465\n",
      "Epoch 303/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7930 - mae: 0.2707 - mse: 0.7172 - val_loss: 2.1614 - val_mae: 0.2618 - val_mse: 2.0856\n",
      "Epoch 304/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2212 - mae: 0.2905 - mse: 1.1454 - val_loss: 2.1403 - val_mae: 0.2259 - val_mse: 2.0645\n",
      "Epoch 305/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9527 - mae: 0.2869 - mse: 0.8768 - val_loss: 2.1067 - val_mae: 0.2351 - val_mse: 2.0308\n",
      "Epoch 306/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8970 - mae: 0.2811 - mse: 0.8212 - val_loss: 1.8764 - val_mae: 0.2228 - val_mse: 1.8006\n",
      "Epoch 307/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9647 - mae: 0.2807 - mse: 0.8888 - val_loss: 2.2361 - val_mae: 0.1859 - val_mse: 2.1602\n",
      "Epoch 308/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9865 - mae: 0.2867 - mse: 0.9106 - val_loss: 1.8415 - val_mae: 0.1825 - val_mse: 1.7656\n",
      "Epoch 309/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7771 - mae: 0.2680 - mse: 0.7012 - val_loss: 1.8773 - val_mae: 0.1953 - val_mse: 1.8014\n",
      "Epoch 310/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7616 - mae: 0.2702 - mse: 0.6857 - val_loss: 1.9324 - val_mae: 0.2185 - val_mse: 1.8565\n",
      "Epoch 311/1000\n",
      "40/40 [==============================] - 18s 445ms/step - loss: 0.8441 - mae: 0.2695 - mse: 0.7682 - val_loss: 1.6361 - val_mae: 0.1977 - val_mse: 1.5602\n",
      "Epoch 312/1000\n",
      "40/40 [==============================] - 8s 183ms/step - loss: 0.8286 - mae: 0.2740 - mse: 0.7527 - val_loss: 1.9263 - val_mae: 0.1849 - val_mse: 1.8504\n",
      "Epoch 313/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8533 - mae: 0.2732 - mse: 0.7774 - val_loss: 1.7499 - val_mae: 0.2032 - val_mse: 1.6739\n",
      "Epoch 314/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7864 - mae: 0.2771 - mse: 0.7104 - val_loss: 2.0240 - val_mae: 0.2136 - val_mse: 1.9481\n",
      "Epoch 315/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.6744 - mae: 0.2639 - mse: 0.5985 - val_loss: 2.1357 - val_mae: 0.1768 - val_mse: 2.0597\n",
      "Epoch 316/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7913 - mae: 0.2635 - mse: 0.7153 - val_loss: 1.9167 - val_mae: 0.1788 - val_mse: 1.8408\n",
      "Epoch 317/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.6956 - mae: 0.2638 - mse: 0.6197 - val_loss: 1.8448 - val_mae: 0.1818 - val_mse: 1.7689\n",
      "Epoch 318/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.6555 - mae: 0.2599 - mse: 0.5795 - val_loss: 1.9136 - val_mae: 0.2167 - val_mse: 1.8377\n",
      "Epoch 319/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8009 - mae: 0.2652 - mse: 0.7249 - val_loss: 1.7482 - val_mae: 0.2119 - val_mse: 1.6722\n",
      "Epoch 320/1000\n",
      "40/40 [==============================] - 7s 182ms/step - loss: 0.8340 - mae: 0.2660 - mse: 0.7580 - val_loss: 1.6913 - val_mae: 0.2144 - val_mse: 1.6153\n",
      "Epoch 321/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8778 - mae: 0.2739 - mse: 0.8017 - val_loss: 2.0464 - val_mae: 0.2146 - val_mse: 1.9704\n",
      "Epoch 322/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7096 - mae: 0.2632 - mse: 0.6335 - val_loss: 1.9168 - val_mae: 0.1678 - val_mse: 1.8407\n",
      "Epoch 323/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7236 - mae: 0.2604 - mse: 0.6476 - val_loss: 1.9365 - val_mae: 0.1745 - val_mse: 1.8605\n",
      "Epoch 324/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8326 - mae: 0.2673 - mse: 0.7565 - val_loss: 1.9206 - val_mae: 0.1689 - val_mse: 1.8445\n",
      "Epoch 325/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7511 - mae: 0.2640 - mse: 0.6751 - val_loss: 2.0872 - val_mae: 0.2333 - val_mse: 2.0111\n",
      "Epoch 326/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.6998 - mae: 0.2619 - mse: 0.6237 - val_loss: 2.0214 - val_mae: 0.1941 - val_mse: 1.9453\n",
      "Epoch 327/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.6529 - mae: 0.2561 - mse: 0.5768 - val_loss: 2.2916 - val_mae: 0.2052 - val_mse: 2.2156\n",
      "Epoch 328/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.6560 - mae: 0.2581 - mse: 0.5799 - val_loss: 2.1378 - val_mae: 0.2340 - val_mse: 2.0617\n",
      "Epoch 329/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7010 - mae: 0.2552 - mse: 0.6249 - val_loss: 2.1713 - val_mae: 0.1663 - val_mse: 2.0953\n",
      "Epoch 330/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8107 - mae: 0.2617 - mse: 0.7346 - val_loss: 2.3037 - val_mae: 0.2827 - val_mse: 2.2276\n",
      "Epoch 331/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8686 - mae: 0.2726 - mse: 0.7925 - val_loss: 2.3220 - val_mae: 0.2035 - val_mse: 2.2459\n",
      "Epoch 332/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8265 - mae: 0.2693 - mse: 0.7504 - val_loss: 2.1678 - val_mae: 0.2176 - val_mse: 2.0917\n",
      "Epoch 333/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0966 - mae: 0.2689 - mse: 1.0205 - val_loss: 2.3233 - val_mae: 0.2475 - val_mse: 2.2471\n",
      "Epoch 334/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.7158 - mae: 0.3069 - mse: 1.6396 - val_loss: 4.3161 - val_mae: 0.3553 - val_mse: 4.2398\n",
      "Epoch 335/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.1647 - mae: 0.3286 - mse: 2.0884 - val_loss: 3.2253 - val_mae: 0.2715 - val_mse: 3.1489\n",
      "Epoch 336/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.3001 - mae: 0.3388 - mse: 2.2236 - val_loss: 2.8951 - val_mae: 0.2829 - val_mse: 2.8186\n",
      "Epoch 337/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.8144 - mae: 0.3201 - mse: 1.7378 - val_loss: 2.4129 - val_mae: 0.2609 - val_mse: 2.3362\n",
      "Epoch 338/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2982 - mae: 0.2960 - mse: 1.2215 - val_loss: 2.2381 - val_mae: 0.1980 - val_mse: 2.1614\n",
      "Epoch 339/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.5747 - mae: 0.3016 - mse: 1.4980 - val_loss: 2.3110 - val_mae: 0.2456 - val_mse: 2.2342\n",
      "Epoch 340/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1648 - mae: 0.2881 - mse: 1.0880 - val_loss: 3.1487 - val_mae: 0.2268 - val_mse: 3.0719\n",
      "Epoch 341/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1610 - mae: 0.2834 - mse: 1.0842 - val_loss: 2.4024 - val_mae: 0.2117 - val_mse: 2.3256\n",
      "Epoch 342/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2059 - mae: 0.2750 - mse: 1.1290 - val_loss: 2.4393 - val_mae: 0.3163 - val_mse: 2.3625\n",
      "Epoch 343/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2874 - mae: 0.2930 - mse: 1.2106 - val_loss: 1.9634 - val_mae: 0.2289 - val_mse: 1.8865\n",
      "Epoch 344/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0347 - mae: 0.2812 - mse: 0.9578 - val_loss: 2.2722 - val_mae: 0.2278 - val_mse: 2.1953\n",
      "Epoch 345/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9405 - mae: 0.2787 - mse: 0.8636 - val_loss: 2.1215 - val_mae: 0.1908 - val_mse: 2.0446\n",
      "Epoch 346/1000\n",
      "40/40 [==============================] - 7s 185ms/step - loss: 0.8268 - mae: 0.2726 - mse: 0.7499 - val_loss: 2.0544 - val_mae: 0.2288 - val_mse: 1.9775\n",
      "\n",
      "=== Fine-tuning no dataset completo ===\n",
      "Epoch 1/1000\n",
      "40/40 [==============================] - 8s 188ms/step - loss: 0.7839 - mae: 0.2705 - mse: 0.7080 - val_loss: 1.9454 - val_mae: 0.1745 - val_mse: 1.8695\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 7s 185ms/step - loss: 0.7396 - mae: 0.2663 - mse: 0.6637 - val_loss: 1.7519 - val_mae: 0.1681 - val_mse: 1.6760\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.6728 - mae: 0.2589 - mse: 0.5969 - val_loss: 1.8400 - val_mae: 0.1801 - val_mse: 1.7641\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8975 - mae: 0.2715 - mse: 0.8216 - val_loss: 1.8199 - val_mae: 0.2058 - val_mse: 1.7439\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7713 - mae: 0.2678 - mse: 0.6954 - val_loss: 2.1217 - val_mae: 0.1901 - val_mse: 2.0458\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8097 - mae: 0.2671 - mse: 0.7338 - val_loss: 1.8079 - val_mae: 0.2183 - val_mse: 1.7320\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7222 - mae: 0.2663 - mse: 0.6463 - val_loss: 1.9811 - val_mae: 0.1828 - val_mse: 1.9052\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9066 - mae: 0.2733 - mse: 0.8306 - val_loss: 2.1875 - val_mae: 0.2419 - val_mse: 2.1115\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7454 - mae: 0.2667 - mse: 0.6694 - val_loss: 1.9878 - val_mae: 0.1834 - val_mse: 1.9118\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8256 - mae: 0.2713 - mse: 0.7495 - val_loss: 1.9834 - val_mae: 0.2064 - val_mse: 1.9074\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8855 - mae: 0.2673 - mse: 0.8095 - val_loss: 1.8969 - val_mae: 0.2057 - val_mse: 1.8208\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 7s 184ms/step - loss: 0.8968 - mae: 0.2737 - mse: 0.8208 - val_loss: 1.8635 - val_mae: 0.2406 - val_mse: 1.7874\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 7s 184ms/step - loss: 0.7685 - mae: 0.2675 - mse: 0.6924 - val_loss: 1.9460 - val_mae: 0.1968 - val_mse: 1.8699\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 7s 184ms/step - loss: 0.7111 - mae: 0.2619 - mse: 0.6350 - val_loss: 1.9327 - val_mae: 0.2201 - val_mse: 1.8567\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.6169 - mae: 0.2632 - mse: 0.5408 - val_loss: 1.8300 - val_mae: 0.2050 - val_mse: 1.7540\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.6760 - mae: 0.2607 - mse: 0.5999 - val_loss: 1.8348 - val_mae: 0.1825 - val_mse: 1.7587\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.6156 - mae: 0.2570 - mse: 0.5395 - val_loss: 2.1486 - val_mae: 0.1771 - val_mse: 2.0726\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7924 - mae: 0.2616 - mse: 0.7163 - val_loss: 1.8461 - val_mae: 0.2251 - val_mse: 1.7700\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7240 - mae: 0.2588 - mse: 0.6479 - val_loss: 1.8259 - val_mae: 0.1798 - val_mse: 1.7498\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8423 - mae: 0.2639 - mse: 0.7662 - val_loss: 1.8613 - val_mae: 0.1828 - val_mse: 1.7852\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.6866 - mae: 0.2597 - mse: 0.6105 - val_loss: 1.8873 - val_mae: 0.1911 - val_mse: 1.8112\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.6141 - mae: 0.2554 - mse: 0.5380 - val_loss: 1.8562 - val_mae: 0.2113 - val_mse: 1.7801\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.6415 - mae: 0.2558 - mse: 0.5654 - val_loss: 1.8763 - val_mae: 0.2178 - val_mse: 1.8002\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.6757 - mae: 0.2581 - mse: 0.5996 - val_loss: 2.0317 - val_mae: 0.2111 - val_mse: 1.9556\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.6653 - mae: 0.2553 - mse: 0.5892 - val_loss: 1.7758 - val_mae: 0.1971 - val_mse: 1.6997\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.5989 - mae: 0.2512 - mse: 0.5228 - val_loss: 1.7836 - val_mae: 0.1737 - val_mse: 1.7075\n",
      "Epoch 27/1000\n",
      "40/40 [==============================] - 17s 443ms/step - loss: 0.6715 - mae: 0.2499 - mse: 0.5953 - val_loss: 1.5999 - val_mae: 0.1966 - val_mse: 1.5237\n",
      "Epoch 28/1000\n",
      "40/40 [==============================] - 8s 184ms/step - loss: 0.7565 - mae: 0.2584 - mse: 0.6804 - val_loss: 2.0075 - val_mae: 0.2077 - val_mse: 1.9314\n",
      "Epoch 29/1000\n",
      "40/40 [==============================] - 7s 184ms/step - loss: 0.8169 - mae: 0.2656 - mse: 0.7408 - val_loss: 1.8555 - val_mae: 0.2018 - val_mse: 1.7793\n",
      "Epoch 30/1000\n",
      "40/40 [==============================] - 7s 184ms/step - loss: 0.7238 - mae: 0.2548 - mse: 0.6477 - val_loss: 2.0583 - val_mae: 0.1854 - val_mse: 1.9822\n",
      "Epoch 31/1000\n",
      "40/40 [==============================] - 7s 184ms/step - loss: 0.9928 - mae: 0.2675 - mse: 0.9167 - val_loss: 2.0168 - val_mae: 0.1842 - val_mse: 1.9406\n",
      "Epoch 32/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9836 - mae: 0.2653 - mse: 0.9074 - val_loss: 1.9198 - val_mae: 0.1726 - val_mse: 1.8437\n",
      "Epoch 33/1000\n",
      "40/40 [==============================] - 7s 184ms/step - loss: 0.6269 - mae: 0.2552 - mse: 0.5507 - val_loss: 1.7066 - val_mae: 0.1954 - val_mse: 1.6305\n",
      "Epoch 34/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.6218 - mae: 0.2546 - mse: 0.5456 - val_loss: 1.6636 - val_mae: 0.2096 - val_mse: 1.5874\n",
      "Epoch 35/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7247 - mae: 0.2572 - mse: 0.6485 - val_loss: 2.1990 - val_mae: 0.1991 - val_mse: 2.1228\n",
      "Epoch 36/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8840 - mae: 0.2649 - mse: 0.8078 - val_loss: 1.9358 - val_mae: 0.3515 - val_mse: 1.8596\n",
      "Epoch 37/1000\n",
      "40/40 [==============================] - 7s 184ms/step - loss: 1.1915 - mae: 0.2771 - mse: 1.1153 - val_loss: 2.2368 - val_mae: 0.1926 - val_mse: 2.1606\n",
      "Epoch 38/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9415 - mae: 0.2751 - mse: 0.8653 - val_loss: 2.2855 - val_mae: 0.2303 - val_mse: 2.2092\n",
      "Epoch 39/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.2798 - mae: 0.2817 - mse: 1.2035 - val_loss: 2.9732 - val_mae: 0.3956 - val_mse: 2.8968\n",
      "Epoch 40/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 2.0762 - mae: 0.3239 - mse: 1.9998 - val_loss: 2.9964 - val_mae: 0.2830 - val_mse: 2.9199\n",
      "Epoch 41/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.8415 - mae: 0.3197 - mse: 1.7650 - val_loss: 2.4478 - val_mae: 0.2886 - val_mse: 2.3712\n",
      "Epoch 42/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8992 - mae: 0.2850 - mse: 0.8226 - val_loss: 2.0940 - val_mae: 0.2978 - val_mse: 2.0174\n",
      "Epoch 43/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.1228 - mae: 0.2893 - mse: 1.0461 - val_loss: 2.2968 - val_mae: 0.2219 - val_mse: 2.2201\n",
      "Epoch 44/1000\n",
      "40/40 [==============================] - 7s 184ms/step - loss: 0.8941 - mae: 0.2720 - mse: 0.8175 - val_loss: 2.1024 - val_mae: 0.2591 - val_mse: 2.0257\n",
      "Epoch 45/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9057 - mae: 0.2695 - mse: 0.8290 - val_loss: 1.7687 - val_mae: 0.2365 - val_mse: 1.6921\n",
      "Epoch 46/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9063 - mae: 0.2632 - mse: 0.8296 - val_loss: 2.1773 - val_mae: 0.2265 - val_mse: 2.1005\n",
      "Epoch 47/1000\n",
      "40/40 [==============================] - 7s 184ms/step - loss: 1.0599 - mae: 0.2734 - mse: 0.9831 - val_loss: 2.6010 - val_mae: 0.2267 - val_mse: 2.5243\n",
      "Epoch 48/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 1.0078 - mae: 0.2755 - mse: 0.9311 - val_loss: 2.3192 - val_mae: 0.2384 - val_mse: 2.2424\n",
      "Epoch 49/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8054 - mae: 0.2607 - mse: 0.7286 - val_loss: 2.1391 - val_mae: 0.2022 - val_mse: 2.0623\n",
      "Epoch 50/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9566 - mae: 0.2643 - mse: 0.8799 - val_loss: 2.4053 - val_mae: 0.1933 - val_mse: 2.3285\n",
      "Epoch 51/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8949 - mae: 0.2607 - mse: 0.8181 - val_loss: 2.4700 - val_mae: 0.2214 - val_mse: 2.3932\n",
      "Epoch 52/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9838 - mae: 0.2694 - mse: 0.9070 - val_loss: 2.2311 - val_mae: 0.2091 - val_mse: 2.1543\n",
      "Epoch 53/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.9354 - mae: 0.2692 - mse: 0.8586 - val_loss: 2.0383 - val_mae: 0.2074 - val_mse: 1.9615\n",
      "Epoch 54/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7407 - mae: 0.2581 - mse: 0.6639 - val_loss: 2.0520 - val_mae: 0.1719 - val_mse: 1.9752\n",
      "Epoch 55/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.8255 - mae: 0.2625 - mse: 0.7486 - val_loss: 1.8729 - val_mae: 0.1825 - val_mse: 1.7960\n",
      "Epoch 56/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7469 - mae: 0.2547 - mse: 0.6700 - val_loss: 1.9270 - val_mae: 0.1937 - val_mse: 1.8501\n",
      "Epoch 57/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.7198 - mae: 0.2593 - mse: 0.6430 - val_loss: 1.7774 - val_mae: 0.1719 - val_mse: 1.7005\n",
      "Epoch 58/1000\n",
      "40/40 [==============================] - 7s 183ms/step - loss: 0.6282 - mae: 0.2536 - mse: 0.5513 - val_loss: 1.7413 - val_mae: 0.1818 - val_mse: 1.6644\n",
      "Epoch 59/1000\n",
      "40/40 [==============================] - 7s 184ms/step - loss: 0.7296 - mae: 0.2518 - mse: 0.6527 - val_loss: 1.8938 - val_mae: 0.1537 - val_mse: 1.8169\n",
      "Epoch 60/1000\n",
      "40/40 [==============================] - 7s 184ms/step - loss: 0.6803 - mae: 0.2522 - mse: 0.6034 - val_loss: 1.8786 - val_mae: 0.1928 - val_mse: 1.8017\n",
      "Epoch 61/1000\n",
      "40/40 [==============================] - 7s 184ms/step - loss: 0.6436 - mae: 0.2508 - mse: 0.5668 - val_loss: 1.7467 - val_mae: 0.1637 - val_mse: 1.6698\n",
      "Epoch 62/1000\n",
      "40/40 [==============================] - 7s 186ms/step - loss: 0.9116 - mae: 0.2640 - mse: 0.8347 - val_loss: 2.0693 - val_mae: 0.2580 - val_mse: 1.9924\n",
      "[INFO] Modelo final salvo em 'final_residual_model.h5'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Exemplo de Previsões no conjunto de Validação ===\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "Amostra 0: [Real lat=1.820, lon=2.060] --> [Pred lat=1.699, lon=2.342]\n",
      "Amostra 1: [Real lat=1.620, lon=1.600] --> [Pred lat=1.482, lon=1.722]\n",
      "Amostra 2: [Real lat=4.700, lon=-0.840] --> [Pred lat=4.797, lon=-0.874]\n",
      "Amostra 3: [Real lat=-1.910, lon=0.490] --> [Pred lat=-2.184, lon=0.619]\n",
      "Amostra 4: [Real lat=4.170, lon=1.610] --> [Pred lat=4.114, lon=1.730]\n",
      "Amostra 5: [Real lat=0.320, lon=-1.180] --> [Pred lat=0.323, lon=-1.069]\n",
      "Amostra 6: [Real lat=2.010, lon=1.500] --> [Pred lat=2.123, lon=1.698]\n",
      "Amostra 7: [Real lat=8.040, lon=-0.840] --> [Pred lat=8.220, lon=-0.607]\n",
      "Amostra 8: [Real lat=5.530, lon=-1.650] --> [Pred lat=5.698, lon=-1.677]\n",
      "Amostra 9: [Real lat=6.000, lon=-2.510] --> [Pred lat=6.105, lon=-2.555]\n",
      "Amostra 10: [Real lat=6.000, lon=-2.510] --> [Pred lat=6.133, lon=-2.506]\n",
      "Amostra 11: [Real lat=2.190, lon=-0.170] --> [Pred lat=2.313, lon=-0.065]\n",
      "Amostra 12: [Real lat=1.200, lon=1.060] --> [Pred lat=1.293, lon=1.265]\n",
      "Amostra 13: [Real lat=1.080, lon=0.020] --> [Pred lat=1.085, lon=0.134]\n",
      "Amostra 14: [Real lat=26.670, lon=-12.680] --> [Pred lat=27.636, lon=-13.225]\n",
      "Amostra 15: [Real lat=1.390, lon=0.680] --> [Pred lat=1.333, lon=0.762]\n",
      "Amostra 16: [Real lat=3.030, lon=0.830] --> [Pred lat=4.069, lon=1.325]\n",
      "Amostra 17: [Real lat=-3.190, lon=-3.320] --> [Pred lat=-3.304, lon=-3.327]\n",
      "Amostra 18: [Real lat=3.410, lon=1.940] --> [Pred lat=3.602, lon=2.235]\n",
      "Amostra 19: [Real lat=0.460, lon=1.430] --> [Pred lat=0.567, lon=1.828]\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Bloco 1: Imports e Configurações\n",
    "###############################################################################\n",
    "import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = DEBUG, 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "\n",
    "import ijson\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks, optimizers, regularizers\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, BatchNormalization, Add\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Verificar se a GPU está disponível e configurada\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"[ERRO] Nenhuma GPU detectada. Certifique-se de que o CUDA está instalado e configurado corretamente.\")\n",
    "else:\n",
    "    print(f\"[INFO] GPUs detectadas: {[gpu.name for gpu in tf.config.list_physical_devices('GPU')]}\")\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "JSON_PATH = \"Dataset_Vetor.json\"\n",
    "TARGET_COLS = [\"diferencalatitudeMetros\", \"diferencalongitudeMetros\"]\n",
    "BATCH_SIZE = 8192\n",
    "EPOCHS = 1000\n",
    "VAL_SPLIT = 0.22\n",
    "MAX_SAMPLES = None  # ou defina para debug (ex: 1_000_000), se quiser testar\n",
    "\n",
    "###############################################################################\n",
    "# Bloco 2: Funções para descobrir chaves e carregar dataset por completo\n",
    "###############################################################################\n",
    "def discover_feature_keys(json_path, max_samples=None):\n",
    "    \"\"\"Descobre todas as chaves (features) disponíveis no JSON.\"\"\"\n",
    "    feature_keys_set = set()\n",
    "    with open(json_path, \"rb\") as f:\n",
    "        parser = ijson.items(f, \"item\")\n",
    "        count = 0\n",
    "        for record in parser:\n",
    "            # Adiciona todas as chaves que não são TARGET_COLS\n",
    "            for k in record.keys():\n",
    "                if k not in TARGET_COLS:\n",
    "                    feature_keys_set.add(k)\n",
    "            count += 1\n",
    "            if max_samples and count >= max_samples:\n",
    "                break\n",
    "    return sorted(feature_keys_set)\n",
    "\n",
    "def dict_to_numpy_fixed(record, feature_keys):\n",
    "    \"\"\"Converte um dicionário de um item do JSON em (X, y) NumPy.\"\"\"\n",
    "    for col in TARGET_COLS:\n",
    "        if col not in record:\n",
    "            return None, None\n",
    "    try:\n",
    "        lat = float(record[\"diferencalatitudeMetros\"])\n",
    "        lon = float(record[\"diferencalongitudeMetros\"])\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "    y_array = np.array([lat, lon], dtype=np.float32)\n",
    "    X_array = np.zeros(len(feature_keys), dtype=np.float32)\n",
    "    for i, k in enumerate(feature_keys):\n",
    "        val = record.get(k, 0.0)\n",
    "        try:\n",
    "            X_array[i] = float(val)\n",
    "        except:\n",
    "            X_array[i] = 0.0\n",
    "    return X_array, y_array\n",
    "\n",
    "def read_full_dataset(json_path, feature_keys, max_samples=None):\n",
    "    \"\"\"\n",
    "    Lê TODO o JSON em memória, retornando X_np (amostras x n_features) e y_np.\n",
    "    \"\"\"\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    count = 0\n",
    "    with open(json_path, \"rb\") as f:\n",
    "        parser = ijson.items(f, \"item\")\n",
    "        for record in parser:\n",
    "            X, y = dict_to_numpy_fixed(record, feature_keys)\n",
    "            if X is not None and y is not None:\n",
    "                dataX.append(X)\n",
    "                dataY.append(y)\n",
    "            count += 1\n",
    "            if max_samples and count >= max_samples:\n",
    "                break\n",
    "    X_np = np.array(dataX, dtype=np.float32)\n",
    "    y_np = np.array(dataY, dtype=np.float32)\n",
    "    return X_np, y_np\n",
    "\n",
    "###############################################################################\n",
    "# Bloco 3: Modelo (com blocos residuais) e função build_model\n",
    "###############################################################################\n",
    "def residual_block(x, units, dr=0.2, l2_reg=1e-6):\n",
    "    \"\"\"Bloco residual com BN + ReLU + Dropout, ajustado para batch size maior.\"\"\"\n",
    "    # Atalho (skip-connection)\n",
    "    sc = x\n",
    "    \n",
    "    # Primeira Dense\n",
    "    x = Dense(units,\n",
    "              kernel_initializer='he_normal',\n",
    "              kernel_regularizer=regularizers.l2(l2_reg),\n",
    "              bias_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dr)(x)\n",
    "    \n",
    "    # Segunda Dense\n",
    "    x = Dense(units,\n",
    "              kernel_initializer='he_normal',\n",
    "              kernel_regularizer=regularizers.l2(l2_reg),\n",
    "              bias_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Soma com o skip-connection\n",
    "    x = Add()([sc, x])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def build_model(input_dim):\n",
    "    inp = Input(shape=(input_dim,))\n",
    "\n",
    "    # Camada inicial\n",
    "    x = Dense(2048,  # Aumente o número de neurônios aqui\n",
    "              activation='relu',\n",
    "              kernel_initializer='he_normal',\n",
    "              kernel_regularizer=regularizers.l2(1e-6),\n",
    "              bias_regularizer=regularizers.l2(1e-6))(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)  # Ajuste o dropout para evitar overfitting\n",
    "\n",
    "    \n",
    "    # Blocos residuais (ajustados)\n",
    "    for _ in range(8):  # Aumente o número de blocos\n",
    "        x = residual_block(x, 2048, dr=0.2, l2_reg=1e-6)  # Aumente o número de neurônios e dropout\n",
    "\n",
    "\n",
    "    # Camada intermediária\n",
    "    x = Dense(1024,  # Aumente o número de neurônios aqui\n",
    "              activation='relu',\n",
    "              kernel_initializer='he_normal',\n",
    "              kernel_regularizer=regularizers.l2(1e-6),\n",
    "              bias_regularizer=regularizers.l2(1e-6))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "\n",
    "    # Saída\n",
    "    out = Dense(2, activation='linear')(x)\n",
    "\n",
    "    model = Model(inp, out)\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(1e-4),\n",
    "        loss='mse',\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "###############################################################################\n",
    "# Bloco 4: MAIN - Carregar tudo em RAM, pré-treino, fine-tuning e gráficos\n",
    "###############################################################################\n",
    "def main():\n",
    "    # 1) Descobrir chaves (features)\n",
    "    feature_keys = discover_feature_keys(JSON_PATH, max_samples=MAX_SAMPLES)\n",
    "    if not feature_keys:\n",
    "        print(\"Nenhuma feature encontrada. Verifique seu JSON.\")\n",
    "        return\n",
    "    print(f\"Total de features: {len(feature_keys)}\")\n",
    "\n",
    "    # 2) Ler TODO o dataset em memória\n",
    "    print(\"[INFO] Lendo dataset inteiro na RAM...\")\n",
    "    X_np, y_np = read_full_dataset(JSON_PATH, feature_keys, max_samples=MAX_SAMPLES)\n",
    "    print(\"[INFO] Formato X_np:\", X_np.shape)\n",
    "    print(\"[INFO] Formato y_np:\", y_np.shape)\n",
    "\n",
    "    # 3) Separar dados em train/val\n",
    "    X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(\n",
    "        X_np, y_np, test_size=VAL_SPLIT, random_state=42\n",
    "    )\n",
    "\n",
    "    # 4) Normalizar\n",
    "    print(\"[INFO] Normalizando dados...\")\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train_np)\n",
    "    X_train_np = scaler.transform(X_train_np)\n",
    "    X_val_np = scaler.transform(X_val_np)\n",
    "\n",
    "    # 5) Construir o modelo\n",
    "    print(\"[INFO] Construindo modelo...\")\n",
    "    model = build_model(input_dim=X_train_np.shape[1])\n",
    "    model.summary()\n",
    "\n",
    "    # Callbacks\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', patience=35, restore_best_weights=True)\n",
    "    ckpt = callbacks.ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor='val_loss')\n",
    "\n",
    "    # 6) Pré-treino offline no subset\n",
    "    #    Neste exemplo, vamos dizer que o \"subset\" são os próprios X_train_np, y_train_np\n",
    "    #    Mas se você quiser um subset ainda menor, basta amostrar parte do X_train_np.\n",
    "    print(\"\\n=== Pré-treino offline no subset ===\")\n",
    "    subset_size = min(len(X_train_np), 500000)  # Exemplo: usar 500k amostras no pretreino\n",
    "    X_sub_np = X_train_np[:subset_size]\n",
    "    y_sub_np = y_train_np[:subset_size]\n",
    "\n",
    "    with tf.device('/GPU:0'):  # Forçar uso da GPU\n",
    "        history_pretrain = model.fit(\n",
    "            X_sub_np, y_sub_np,\n",
    "            validation_data=(X_val_np, y_val_np),\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks=[es, ckpt],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    # 7) Fine-tuning no dataset completo (ou seja, todo X_train_np) usando GPU\n",
    "    print(\"\\n=== Fine-tuning no dataset completo ===\")\n",
    "    with tf.device('/GPU:0'):\n",
    "        history_finetune = model.fit(\n",
    "            X_train_np, y_train_np,\n",
    "            validation_data=(X_val_np, y_val_np),\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks=[es, ckpt],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    # Salvar o modelo final\n",
    "    model.save(\"final_residual_model.h5\")\n",
    "    print(\"[INFO] Modelo final salvo em 'final_residual_model.h5'\")\n",
    "\n",
    "    ############################################################################\n",
    "    # Bloco 5: Gráficos do treinamento (Loss vs. Épocas)\n",
    "    ############################################################################\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(\"Evolução do Treinamento - Pré-treino vs Fine-tuning (Loss)\")\n",
    "    plt.plot(history_pretrain.history['loss'], label='pretrain_loss')\n",
    "    plt.plot(history_pretrain.history['val_loss'], label='pretrain_val_loss')\n",
    "    plt.plot(history_finetune.history['loss'], label='finetune_loss')\n",
    "    plt.plot(history_finetune.history['val_loss'], label='finetune_val_loss')\n",
    "    plt.xlabel(\"Épocas\")\n",
    "    plt.ylabel(\"Loss (MSE)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    ############################################################################\n",
    "    # Bloco 6: Exemplo de predição vs. real\n",
    "    ############################################################################\n",
    "    print(\"\\n=== Exemplo de Previsões no conjunto de Validação ===\")\n",
    "    preds = model.predict(X_val_np[:20])\n",
    "    for i in range(20):\n",
    "        real_lat, real_lon = y_val_np[i]\n",
    "        pred_lat, pred_lon = preds[i]\n",
    "        print(f\"Amostra {i}: [Real lat={real_lat:.3f}, lon={real_lon:.3f}] --> [Pred lat={pred_lat:.3f}, lon={pred_lon:.3f}]\")\n",
    "\n",
    "###############################################################################\n",
    "# Execução\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Garantir que o TensorFlow use a GPU (se disponível)\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Configurar TensorFlow para usar apenas uma GPU (opcional)\n",
    "            tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "            tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "            print(f\"[INFO] Usando GPU: {gpus[0].name}\")\n",
    "        except RuntimeError as e:\n",
    "            print(\"[ERRO] Falha ao configurar GPU:\", e)\n",
    "\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
